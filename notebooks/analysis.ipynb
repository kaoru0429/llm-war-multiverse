{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一次 LLM 架構戰爭：數據分析 (C-01)\n",
    "\n",
    "本筆記本分析了 2023-2025 年間主要 LLM 的關鍵指標，以量化架構戰爭的趨勢。\n",
    "\n",
    "## 1. 數據加載與預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# 讀取數據\n",
    "df = pd.read_csv('../data/llm_models.csv')\n",
    "df['Release_Date'] = pd.to_datetime(df['Release_Date'])\n",
    "df.sort_values('Release_Date', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 參數量演變：Dense vs MoE\n",
    "\n",
    "觀察總參數 (Total Parameters) 與實際啟動參數 (Active Parameters) 隨時間的變化，可以看出 MoE 架構如何打破了「參數越大越好」的迷思。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x='Release_Date', y='Total_Parameters_B', \n",
    "                 size='Active_Parameters_B', color='Architecture', \n",
    "                 hover_name='Model', text='Model',\n",
    "                 title='LLM 參數演變 (圓點大小代表啟動參數)')\n",
    "fig.update_traces(textposition='top center')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 訓練成本革命\n",
    "\n",
    "DeepSeek 與 Kimi 的出現徹底改變了成本結構。下圖展示了不同模型的訓練成本對比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df, x='Model', y='Training_Cost_USD_M', color='Architecture',\n",
    "             title='各模型訓練成本對比 (百萬美元)',\n",
    "             text='Training_Cost_USD_M')\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 結論\n",
    "\n",
    "從數據中可以清晰地看到：\n",
    "1. **MoE 的效率優勢**：雖然總參數量持續增長（達到 1T+），但啟動參數保持在 30B-50B 區間，保證了推理速度。\n",
    "2. **成本崩塌**：2025 年的頂級模型訓練成本僅為 2023 年的 5%，這是架構創新（MoE）帶來的紅利。\n",
    "3. **Meta 的轉向**：Llama 4 的數據點清楚地標誌著開源界向 MoE 的全面倒戈。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
